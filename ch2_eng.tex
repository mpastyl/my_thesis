\selectlanguage{english}
\def\<#1>{\textit{#1}}

\chapter{FIFO Queues}
\section{Introduction}

FIFO Queues are one of the most widely used data structures. They have been studied thoroughly and are used in many projects. In order to maintain the coherence of the data structure in a multithreaded environment, synchronization between threads is needed. Queues are a structure that, by its nature, allows low levels of concurrency, since all reads and writes are applied on Head and Tail, rendering these locations as hotspots. Therefore, we expect low scalability, as the number of parallel threads increases.

However, in many applications, there is the need of having multiple threads, communicating through a shared queue, that is often needed to serve millions of operations per second. For this reason, concurrent FIFO Queue implementations attract theoretical, as well as practical interest. 

Given the low level parallelization offered by the structure, the problem of high performance, essentially becomes the problem of finding a low cost synchronization scheme, as well as achieving better cache utilization.

\subsection{Global Lock}
In our first implementation we adopted a naive, coarse grained strategy. We introduce a global lock, which every thread is trying to set at the beginning of an enqueue or a dequeue. As expected,  performance is disappointing and the implementation does not scale. The reason is that, only one thread can execute an operation on the structure every given time, while all the other threads spin on the lock. Even after we implemented and used a TTES( Test an TEst and Set) lock that reduces traffic due to the cache coherence protocol, delays were still high.

Another characteristic behavior is that throughput declines dramatically when the number of parallel threads exceeds the number of available cores. In this case, more than one threads share the same core and if a thread holding the lock is scheduled out, no other thread is advancing until that threads regains the cpu and unsets the lock. This is a general problem with locking implementations: If a thread inside the critical section is delayed, the progress of all other threads is halted and performance suffers significantly.

%global benchmark here??

For this reason, in the field of data structures in general and FIFO Queues in particular, great effort has been applied to come up with sustainable, lock free implementations. The first and most well known, successful implementation of a lock-free FIFO Queue is from Michael and Scott \cite{msqueue} and it serves as a base line for all further efforts.

\section{Michael Scott Queue}
\subsection{Description}

The data structure is implemented as a simply linked list, with a pointer Head referencing the start of the list, where dequeues are applied and a pointer Tail at the end where we can add new nodes. The node pointed by Head is considered a dummy node  and is used to ensure that the list is never left empty.

In its core, the algorithm uses atomic operations ( in particular Compare And Swap operations) to atomically modify the appropriate pointers. Every time, we run checks to make sure that we have a consistent view of the pointers we are trying to modify.

As depicted below, an enqueue requires 2 atomic operations: one to link the last node with the new node we are trying to insert and one to swap the Tail pointer to the new node.

% εικόνα 1 από το optimistic paper
\begin{figure}
 \centering
  \includegraphics[scale=0.4]{msqueue_struct.png}
\caption{ The basic layout of the link list used in Michael Scott implementation}
\end{figure}

On the other hand, we need only one C-A-S on the value of Head to perform a dequeue.

\subsection{Challenges}
At any given time during the execution of a thread, the values of Head and Tail can change unexpectedly, causing the atomic operations to fail and the execution to start over from the top: read the new value of Head/Tail and try to change it atomically. Moreover, in order for an enqueue to finish, both two required C-A-Ss need to succeed. This in turn makes it possible for the new node to be added to the list, without updating the value of Tail accordingly, i.e. Tail doesn't point to the last node. For  this reason, during enqueue or dequeue, it is necessary to check for this inconsistency and correct it.

One of the problems that occurs during the implementation of this algorithm in particular and of algorithms that use C-A-S in general, is the so called ABA problem. In summary, the problem is described by the following scenario: A process observes that a memory location is in a state A and then is halted for a while. In the meantime, another process alters the state of the memory location to B, then back to A again. The initial process will find that the state of the memory location is A and a CAS will succeed, without knowing that the state has changed in the meantime. This problem is related to the lack of a garbage collector that would ensure that we could not release a memory segment that is still being referenced by a thread.

In order to solve the problem, we chose to use modification counters, which we increase on every successful C-A-S and we incorporate them along with every pointer on the data structure. Atomic operations are now performed, not on the pointer but on the pair <pointer, modification counter> which is now treated as a single variable. Thus, we now have to treat pointers in a non traditional way, extracting them from the variable using bit shifting. Alternatively,  there have been put forward many ways to solve the ABA problem, for example with the use of reference counters, that doesn't require merging counters and pointers.

\subsection{results}
The result is a lock-free implementation,  that does not require central locking of the data structure and allows all threads to advance. Performance is not affected by random delays that a thread can have. It thus improved performance compared to the global lock implementation, especially when the number of concurrent threads exceeds the number of available cores.

% isws grafikh gia msqueue

%lock-freeness/orthothta?


\section{Optimistic Queue}

\subsection{introduction}
One of the drawbacks of lock-free approaches is that, each time an atomic operation fails, the execution start back from the top. Moreover, failed atomic operations are costly, due to the synchronization barrier they introduce. Especially in Michael Scott Queue, since both two atomic operations need to be successful in order to complete an enqueue, it is quite common for a thread to repeat execution again and again until it is done correctly. For this reason, we would like to reduce the number of synchronization points ,e.g. the number of atomic operations.

A solution to this problem is introduced by the next algorithm we implemented, by Ladan-Mozes and Shavit \cite{optimistic}. This implementation follows an optimistic approach ( which is why we will refer to this implementation as optimistic queue), in a sense that it runs quickly on the optimistic case where there is no conflict and leaves the costly operations for the case where an inconsistency is spotted. In particular, one of the two C-A-S operations, during enqueue, is replaced with a simple local store, making sure that we correct the data structure in case it is inconsistent.

\subsection{implementation}
Practically, the link list becomes a doubly linked list, with the "next" directions being from Tail to Head. In this way, we only need a single C-A-S to Tail to make it point to the new node, in order to successfully complete an enqueue. However, in order to have access to node from head when we dequeue, we need pointers in the reverse order, as seen below.

%figure 2 apo optimistic paper
\begin{figure}
 \centering
  \includegraphics[scale=0.6]{optimistic_struct.png}
 \caption{ The basic layout of the doubly linked list used in the Optimistic Queue}
\end{figure}

Pointers in both directions are updated with simple local stores , without synchronization, which makes inconsistencies possible, in the "prev" direction. For this reason, as soon as an inconsistency is spotted, function FixList is called to traverse the list and fix all the pointers. However, the reason for inconsistencies in the "prev" direction are the long delays a single thread might take and not contention. Therefore, we expect the number of calls to FixList to remain low, even when the number of parallel threads increases.

Inserting a new node in the list includes 3 steps:
1) Set the next pointer of the new node we are trying to insert
2) Compare And Swap on Tail, to make it point to the new node
3) Change the prev pointer of the next node.

\subsection{ABA and consistency}

In order to avoid the ABA problem and spot inconsistencies, this implementation also uses modification counters, that are merged along with the pointers and are incremented in every successful C-A-S

Any thread might take arbitrary time  between steps 2 and 3 and during this time more nodes might be inserted in the queue. Note however that, every time a node is successfully inserted in the queue(after successful C-A-S), the modification counter to be inserted next is incremented by one. Thus, pointers of consecutive nodes in the queue, will have consecutive modification counters. In this way, during a dequeue, if a prev pointer does not have the expected modification counter, FixList is called and the pointers are repaired.

Note that there must be special care taken to ensure that there is always one dummy node in the list and Tail never goes past that node.

\subsection{Failed C-A-S operations}

The next diagram compares the number of C-A-S operations( both successful and failed) needed to execute 1 million pairs of enqueue/dequeue, across these two lock-free implementations. We can see that the optimistic approach, as promised, requires less C-A-Ss and has, in total less costly, failed C-A-Ss as the level of concurrency increases.

%διαγραμμα για failed CAS
\begin{figure}
 \centering
  \includegraphics[scale=0.5]{failed_cas.png}
\caption{Total successful and failed C-A-Ss for the two lock-free implementations}
\end{figure}

\section{Flat Combining}
\subsection{Introduction}

The two proceeding implementations follow a fine thread approach, where every thread has access to the data structure and they are trying to achieve performance through high parallelization. As more treads are free to operate on the data structure, performance is expected to be better that locking approaches that block the progress of some threads. We next present the principles of flat combining, a programming approach by Hendler, Incxe and Shavit \cite{flat_combining} that goes against the above mentioned statements.

In particular, the authors claim that the point at which the cost of synchronization between threads exceeds the benefit from high parallelization, is at a lower lever of concurrency than expected. Flat combining is based on a synchronization scheme where each thread locks  the data structure in an extremely low-cost way, gathers information on the operations trying to be executed on the queue by the other threads and then does the operations in their place. The result is an implementation with low synchronization cost and better cache performance, overcoming the drawbacks of blocking and low parallelization.

\subsection{Implementation}

Flat combining is a layer of abstraction that can be used over a sequential structure and its basic functions is the following:
1) Every thread publishes the operations it is trying to perform on the structure, along with any parameters, on its corresponding public record. These records can be in an array, for fast read/write or in linked list with dynamic size, proportional to the number of active threads.
2) Every thread checks the state of the locks and if it finds it unlocked, it tries only once to atomically set the lock. If the lock was locked already, the thread spins in its public record, waiting for a response.
3) If it takes the lock, this thread is considered the new combiner. It then traverses the public records and executes the request on the data structure, one by one, writing back the results. Finally the thread releases the lock.

% isws kana sxhmataki  gia th domh  san auto sto paper

\subsection{Implementation characteristics and Benefits}
From the way flat combining works, certain advantages can be concluded:
1)Since only the combiner has access to the structure, operations can be optimized with the best sequential algorithm, without minding synchronization and concurrency. This even allows us to implement concurrent structures, such as pairing heaps, that are otherwise difficult to implement using fine grain synchronization, by applying flat combining over already existent sequential data structures.
2) in some cases, the combiner can use some smart way to group the request and make the access to the structure faster and more efficient. For example, in concurrent stacks, a combiner can perform elimination between a concurrent enqueue and a concurrent dequeue. In our queue implementation, we used fat nodes to group data, as we explain later on.
3) The combining way of accessing the structure can lead to better cache utilization.
4) Using a global lock to isolate the structure makes programming, as well as debugging, much easier.

It is also important to note that flat combining, as a layer of abstraction that ensures synchronization, can bee used as it is over different data structured. Of course, not all data structures are benefited by flat combining. For example, if the cost of a single operation in a search tree is \textgreek{Θ} (logn), using flat combining, the cost of k operations is in general \textgreek{Θ} (klogn), while we could  use parallel threads that execute operations independently on different parts of the tree in \textgreek{Θ} (longn) total time.

FIFO Queues in particular, seem to benefit by flat combining, given that they already allow low levels of concurrency ( only two access points). In our implementation, we organized public records in an array and used fat nodes, where every node can hold up to 16 values. Therefore, we only need to add one node in the queue and swap the Tail pointer only once, for every 16 values inserted. We also used two independent instances of flat combining, one for enqueues and one for dequeues, in order to exploit the maximum concurrency allowed.

\subsection{First results}
 %grafikes gia fc se sxesh me ta alla
 
\subsection{Optimizations}
%1 dedicated combiner
From the analysis of the algorithm we can deduct that every thread can, in theory, access the queue and alter its state. That means that the nodes of the queue can be in the cache memory of any thread, which in turn leads to increased cache misses and long delays.

For this reason, we alter the existing implementation, so that only a given thread for each operation(enqueue/dequeue) can become a combiner. Essentially, all threads are being server by only two specific threads. These two threads have less cache misses when they access the structure.

The result is a constant improvement in performance, that corresponds  to less cache misses and better locality. Specifically on dunnington, this improvement is even more apparent.

%2 hybird
We next  studied the behavior of these algorithms in NUMA architectures. From the general performance of these algorithms, it is obvious that when threads are no longer contained in one NUMA node, performance suffers significantly, due to the fact that threads no longer share the same L2 cache. Moreover, atomic operations and specifically Compare-And-Swap), are much more costly when they need to synchronize threads that belong to different NUMA nodes.

The previously implemented flat combining algorithm is not NUMA-aware and cannot deal with the problems mentioned above. For this reason, we designed and implemented a hybrid implementation that combines previous algorithms, taking architecture into account, in order to achieve better performance.

The implementation that was eventually chosen  consists of two stages. In the first stage, we use flat combining in every NUMA node. Threads of the same node are synchronized to come up with a combiner. In the second stage, combiners from each node are further synchronized to access the queue. In the second stage we tried several synchronization schemes ( Michael-Scott queue, a second level of flat combining). Eventually, the best performance for the given architecture was achieved by a simple TTAS global lock.

The result was a further improvement of the performance,

